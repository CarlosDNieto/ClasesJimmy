{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Miércoles **29/01/2020**\n",
    "\n",
    "---\n",
    "\n",
    "## Comandos:\n",
    "* **``jps:``** Nos dice que procesos Java tenemos\n",
    "* **``ls:``** Nos dice los elementos en el directorio que estamos\n",
    "* **``./start-dfs.sh:``** Inicializa NameNodes, DataNodes y SecondaryNameNodes\n",
    "* **``hdfs dfs -mkdir /NombreDeLaCarpeta:``** Para crear un directorio HDFS\n",
    "* **``hdfs dfs -ls /:``** Para buscar los elementos que hay en hadoop\n",
    "* **``hdfs dfs -rmr /NombreDeCarpeta:``** elimina la carpeta r.- de recursivo\n",
    "* **``hdfs dfs -put local(ruta) hdfs:``** subir un archivo\n",
    "* **``hdfs dfs -get hdfs local:``** para bajar un archivo\n",
    "\n",
    "\n",
    "## Hadoop\n",
    "\n",
    "> ya nos está preparando las máquinas virtuales\n",
    "\n",
    "* Hadoop es un framework de big data, que permite procesamiento distribuido de grandes cantidades de datos entre clusteres de computadoras **(cómputo distribuido)**.\n",
    "\n",
    "* Un framework son soluciones a problemas ya conocidos, ejemplos:\n",
    "    * Los que se dedican a desarrollo web, para hacer un sistema web se tiene que hacer un **login** entonces usamos un framework como **DJango**, **Flask**, etc para no reinventar algo que ya se tiene para hacer un login.\n",
    "\n",
    "* Está diseñado para escalarse de un único servidor a miles de máquinas.\n",
    "\n",
    "* Está diseñado para detectar y manejar fallos, ofreciendo **alta disponibilidad (HA High Availability)**\n",
    "    * Problema **CAP** que no puedes tener alta disponibilidad. Lo vamos a ver más adelante.\n",
    "    \n",
    "### Módulos de Hadoop:\n",
    "* Hadoop Common\n",
    "* **Hadoop Distributed File System (HDFS):** Storage.\n",
    "* **Hadoop MapReduce:** Utiliza YARN para procesamiento.\n",
    "* Hadoop YARN: \n",
    "* Hadoop Ozone\n",
    "* Hadoop Submarine\n",
    "    \n",
    "### Modos de Hadoop:\n",
    "\n",
    "* **Modo Standalone:** Modo no distribuido en el que hadoop corre como un único proceso Java.\n",
    "    * NameNode, DataNode y SecondaryNameNode corren en un proceso Java igual.\n",
    "* **Modo Pseudo-Distribuido:** Hadoop corre en un único nodo donde cada dominio Hadoop corre en un proceso Java separado\n",
    "    * NameNode, DataNode y SecondaryNameNode corren en diferentes procesos Java.\n",
    "> Vamos a trabajar el modo pdeudo distribuído\n",
    "\n",
    "### HDFS\n",
    "\n",
    "HDFS es un sistema de ficheros distribuido de Hadoop, tolerante a fallos y está diseñado para implementarse en software de bajo costo.\n",
    "\n",
    "> Un **server** es una máquina que está diseñada para funcionar indefinidamente con los debidos recursos.\n",
    "\n",
    "* Si tu le subes un archivo a hadoop, éste lo va a dividir en los diferentes nodos. \n",
    "    * Lo recomendable es que sean 3 diferentes nodos.\n",
    "    * Tamaño de nodos: 128 Megas.\n",
    "    \n",
    "* **Estructuras de datos resilentes distribuidas (RDD):** Estas cosas están diseñadas para trabajar en ambientes distribuídos\n",
    "\n",
    "### DataNode y NameNode\n",
    "\n",
    "HDFS tiene una arquitectura maestro-esclavo. Un clúster HDFS consta de: \n",
    "\n",
    "* un único **NameNode:** un servidor maestro que administra y regula el acceso a los archivos por parte de los clientes.\n",
    "\n",
    "* Varios **DataNode:** Generalmente uno por nodo en el clúster que administran el almacenamiento. Internamente un archivo se divide en uno o más bloques, por defecto 3.\n",
    "\n",
    "Hadoop nos dá una interfaz web.\n",
    "\n",
    "> **Hive** nos permite a interactuar en una interfaz como SQL para trabajar con archivos HDFS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Jueves 30/01/2020\n",
    "\n",
    "---\n",
    "\n",
    "### Arango DB, Mongo DB\n",
    "\n",
    "Un sistema distribuido es un conjunto de computadoras independientes que se presentan como un sistema único coherente.\n",
    "\n",
    "> investigar problema **CAP** (problema de disponibilidad)\n",
    "\n",
    "* Un banco utiliza un **MainFrame** para resolver su problema CAP quieren hacer un escalamiento vertical.\n",
    "\n",
    "> Un **MainFrame** una supercomputadora con una capacidad de procesamiento enorme\n",
    "\n",
    "* Hay escabilidad horizontal y vertical.\n",
    "\n",
    "### Protocolos de comunicación entre computadoras\n",
    "\n",
    "* **http**\n",
    "* **udp**\n",
    "* **ftp**\n",
    "* **ssh** \n",
    "\n",
    "> **ssh** es el protocolo que utuliza HDFS para comunicar nodos.\n",
    "\n",
    "* $2^{16}$ es el número de puertos totales\n",
    "\n",
    "### Cargar un archivo JSON en HDFS\n",
    "\n",
    "1. Creamos un directorio de HDFS con ``hdfs dfs -mkdir /bigdata``\n",
    "\n",
    "\n",
    "2. Subimos nuestro archivo con ``hdfs dfs -put rutadelarchivo /bigdata``\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cómo leer un json\n",
    "\n",
    "import json\n",
    "with open ('nombredelarchivo','r') as f:\n",
    "    datos = json.load(f)\n",
    "\n",
    "# Conectarnos con un cliente HDFS\n",
    "from hdfs import InsecureClient\n",
    "client = InsecureClient('http://localhost:9870')\n",
    "\n",
    "#contenidos del cliente\n",
    "content = client.content('/')\n",
    "\n",
    "#listar lo que tiene en la raíz\n",
    "fnames = client.list('/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
