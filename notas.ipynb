{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Miércoles **29/01/2020**\n",
    "\n",
    "---\n",
    "\n",
    "## Comandos:\n",
    "* **``jps:``** Nos dice que procesos Java tenemos\n",
    "* **``ls:``** Nos dice los elementos en el directorio que estamos\n",
    "* **``./start-dfs.sh:``** Inicializa NameNodes, DataNodes y SecondaryNameNodes\n",
    "* **``hdfs dfs -mkdir /NombreDeLaCarpeta:``** Para crear un directorio HDFS\n",
    "* **``hdfs dfs -ls /:``** Para buscar los elementos que hay en hadoop\n",
    "* **``hdfs dfs -rmr /NombreDeCarpeta:``** elimina la carpeta r.- de recursivo\n",
    "* **``hdfs dfs -put local(ruta) hdfs:``** subir un archivo\n",
    "* **``hdfs dfs -get hdfs local:``** para bajar un archivo\n",
    "\n",
    "\n",
    "## Hadoop\n",
    "\n",
    "> ya nos está preparando las máquinas virtuales\n",
    "\n",
    "* Hadoop es un framework de big data, que permite procesamiento distribuido de grandes cantidades de datos entre clusteres de computadoras **(cómputo distribuido)**.\n",
    "\n",
    "* Un framework son soluciones a problemas ya conocidos, ejemplos:\n",
    "    * Los que se dedican a desarrollo web, para hacer un sistema web se tiene que hacer un **login** entonces usamos un framework como **DJango**, **Flask**, etc para no reinventar algo que ya se tiene para hacer un login.\n",
    "\n",
    "* Está diseñado para escalarse de un único servidor a miles de máquinas.\n",
    "\n",
    "* Está diseñado para detectar y manejar fallos, ofreciendo **alta disponibilidad (HA High Availability)**\n",
    "    * Problema **CAP** que no puedes tener alta disponibilidad. Lo vamos a ver más adelante.\n",
    "    \n",
    "### Módulos de Hadoop:\n",
    "* Hadoop Common\n",
    "* **Hadoop Distributed File System (HDFS):** Storage.\n",
    "* **Hadoop MapReduce:** Utiliza YARN para procesamiento.\n",
    "* Hadoop YARN: \n",
    "* Hadoop Ozone\n",
    "* Hadoop Submarine\n",
    "    \n",
    "### Modos de Hadoop:\n",
    "\n",
    "* **Modo Standalone:** Modo no distribuido en el que hadoop corre como un único proceso Java.\n",
    "    * NameNode, DataNode y SecondaryNameNode corren en un proceso Java igual.\n",
    "* **Modo Pseudo-Distribuido:** Hadoop corre en un único nodo donde cada dominio Hadoop corre en un proceso Java separado\n",
    "    * NameNode, DataNode y SecondaryNameNode corren en diferentes procesos Java.\n",
    "> Vamos a trabajar el modo pdeudo distribuído\n",
    "\n",
    "### HDFS\n",
    "\n",
    "HDFS es un sistema de ficheros distribuido de Hadoop, tolerante a fallos y está diseñado para implementarse en software de bajo costo.\n",
    "\n",
    "> Un **server** es una máquina que está diseñada para funcionar indefinidamente con los debidos recursos.\n",
    "\n",
    "* Si tu le subes un archivo a hadoop, éste lo va a dividir en los diferentes nodos. \n",
    "    * Lo recomendable es que sean 3 diferentes nodos.\n",
    "    * Tamaño de nodos: 128 Megas.\n",
    "    \n",
    "* **Estructuras de datos resilentes distribuidas (RDD):** Estas cosas están diseñadas para trabajar en ambientes distribuídos\n",
    "\n",
    "### DataNode y NameNode\n",
    "\n",
    "HDFS tiene una arquitectura maestro-esclavo. Un clúster HDFS consta de: \n",
    "\n",
    "* un único **NameNode:** un servidor maestro que administra y regula el acceso a los archivos por parte de los clientes.\n",
    "\n",
    "* Varios **DataNode:** Generalmente uno por nodo en el clúster que administran el almacenamiento. Internamente un archivo se divide en uno o más bloques, por defecto 3.\n",
    "\n",
    "Hadoop nos dá una interfaz web.\n",
    "\n",
    "> **Hive** nos permite a interactuar en una interfaz como SQL para trabajar con archivos HDFS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Jueves 30/01/2020\n",
    "\n",
    "---\n",
    "\n",
    "### Arango DB, Mongo DB\n",
    "\n",
    "Un sistema distribuido es un conjunto de computadoras independientes que se presentan como un sistema único coherente.\n",
    "\n",
    "> investigar problema **CAP** (problema de disponibilidad)\n",
    "\n",
    "* Un banco utiliza un **MainFrame** para resolver su problema CAP quieren hacer un escalamiento vertical.\n",
    "\n",
    "> Un **MainFrame** una supercomputadora con una capacidad de procesamiento enorme\n",
    "\n",
    "* Hay escabilidad horizontal y vertical.\n",
    "\n",
    "### Protocolos de comunicación entre computadoras\n",
    "\n",
    "* **http**\n",
    "* **udp**\n",
    "* **ftp**\n",
    "* **ssh** \n",
    "\n",
    "> **ssh** es el protocolo que utuliza HDFS para comunicar nodos.\n",
    "\n",
    "* $2^{16}$ es el número de puertos totales\n",
    "\n",
    "### Cargar un archivo JSON en HDFS\n",
    "\n",
    "1. Creamos un directorio de HDFS con ``hdfs dfs -mkdir /bigdata``\n",
    "\n",
    "\n",
    "2. Subimos nuestro archivo con ``hdfs dfs -put rutadelarchivo /bigdata``\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cómo leer un json\n",
    "\n",
    "import json\n",
    "with open ('nombredelarchivo','r') as f:\n",
    "    datos = json.load(f)\n",
    "\n",
    "# Conectarnos con un cliente HDFS\n",
    "from hdfs import InsecureClient\n",
    "client = InsecureClient('http://localhost:9870')\n",
    "\n",
    "#contenidos del cliente\n",
    "content = client.content('/')\n",
    "\n",
    "#listar lo que tiene en la raíz\n",
    "fnames = client.list('/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Viernes 31/01/2019\n",
    "\n",
    "---\n",
    "\n",
    "> Linus Torvald - creador del núcleo de Linux y Git.\n",
    "\n",
    "En un modo **stand-alone** los demonios corren en un mismo nodo, en un modo **psudo-distribuido** los demonios corren en diferentes nodos.\n",
    "\n",
    "* **Programación Orientada a Objetos (OOP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conexion(url):\n",
    "    \"\"\"\n",
    "    url --string no null\n",
    "    \n",
    "    Esta función logra la conexión con hdfs\n",
    "    \"\"\"\n",
    "        try:\n",
    "            client = InsecureClient(url)\n",
    "            return client\n",
    "        except:\n",
    "            print('Ocurrio un error al verificar con el host, favor de verificar la url')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_directorio(pathhdfs):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        pathcreado = client.makedirs(pathhdfs)\n",
    "        print('Se creo el directorio '+pathhdfs)\n",
    "        return pathcreado\n",
    "    except:\n",
    "        print('Ocurrió un error, favor de verificar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_archivo(pathhdfs,local='rutadelarchivo.json'):\n",
    "    try:\n",
    "        file=client.upload(pathhdfs,local)\n",
    "        return file\n",
    "    except:\n",
    "        print(\"Hubo un error\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Martes 04/02/2020\n",
    "\n",
    "---\n",
    "\n",
    "### POO\n",
    "* Representaciones múltiples\n",
    "* Encapsulado\n",
    "* Subtipo\n",
    "* Herencia\n",
    "* Recursión interna\n",
    "\n",
    "> La programación orientada a objetos es un paradigma. Un paradigma es la manera de resolver un problema.\n",
    "\n",
    "* Un **objeto** es una estructura de datos que encapsula cierto estado interno ofreciendo acceso a este mediante una colección de métodos.\n",
    "\n",
    "\n",
    "* El estado interno se caracteriza mediante atributos o variables de instancia.\n",
    "\n",
    "\n",
    "### Representaciones múltiples\n",
    "Cuando una operación es invocada en un objeto, el objeto mismo determina que código ejecutar. Dos objetos responden al mismo conjunto de operaciones (es decir, tienen las misma interfaz) pueden usar representaciones completamente diferentes, siempre y cuando cuenten con una implementación de las operaciones que funciones con la representación particular.\n",
    "\n",
    "### Encapsulado\n",
    "La representación interna de un objeto se oculta, por lo general , fuera de la definición del objeto mismo. Esto quiere decir que solo los métodos del objeto pueden inspeccionar o manipular sus campos. Los cambios a la representación interna de un objeto afectan solo a una región pequeña y fácilmente identificable del programa. Esta restricción mejora considerablemente el mantenimiento y legibilidad de sistemas grandes.\n",
    "\n",
    "### Subtipo (herencia de interfaces)\n",
    "El tipo de un objeto, es decir su interfaz, es simplemente el cojunto de nombres y tipos de sus operaciones. \n",
    "\n",
    "### Herencia de clases\n",
    "Los objetos que comparten partes de sus interfaces tambiém a menudo comparten algnos de sus comportamientos, y nos gustaría implementar estos comportamientos comunees solo una vez. La mayoría de lenguajes orientados a obetos permiten este reuso de comportamientos mediante estructuras llamadas clases las cuales proporcionan plantillas a partir de las cuales se instancian los objetos.\n",
    "\n",
    "### Recursión Interna.\n",
    "Un método de un objeto puede llamar a otro dentro del mismo objeto con ayuda de las variables especiales self o this.\n",
    "\n",
    "> PEP8 <- convenciones de python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo de clases\n",
    "\n",
    "# módulo alumno.py\n",
    "\n",
    "class Alumno:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self,noCuenta,nombre):\n",
    "        self.noCuenta = noCuenta\n",
    "        self.nombre = nombre\n",
    "        \n",
    "    def get_noCuenta(self):\n",
    "        return self.noCuenta\n",
    "    \n",
    "    def set_noCuenta(self,noCuenta):\n",
    "        self.noCuenta = noCuenta\n",
    "        \n",
    "    # interfaz\n",
    "    def estudia()\n",
    "    \n",
    "# para pasar la herencia\n",
    "class Actuario(Alumno):\n",
    "    def __init__(self,depto,noCuenta,nombre):\n",
    "        Alumno.__init__(self,noCuenta,nombre)\n",
    "        self.depto = depto\n",
    "    \n",
    "    def apartarLugares(self,noLugares):\n",
    "        print(\"se apartaron \" + str(noLugares))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Miércoles 05/02/2020\n",
    "\n",
    "---\n",
    "\n",
    "### SecondaryNameNode\n",
    "Es un proceso que se encarga de de contener cierta información y hace procesos sobre la metadata. \n",
    "\n",
    "\n",
    "### Metadata\n",
    "* Edits\n",
    "* **edits_inprogress:** especie de captura en un tiempo determinado sobre HDFS\n",
    "* fsimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Miércoles 19/02/2020\n",
    "---\n",
    "\n",
    "Microservicios. \n",
    "\n",
    "\n",
    "### Cómo cargar un JSON en ArangoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyArango.connection as arc\n",
    "import numpy as np\n",
    "import json as js\n",
    "\n",
    "# Establecemos la conexión con el servidor y la base de datos\n",
    "conn = arc.Connection(arangoURL=\"http://159.203.120.22:8529/\" ,username=\"alumno1\", password=\"alumno1bigdatafc20202\")\n",
    "db = conn[\"bigdatafc1\"]\n",
    "\n",
    "# Creamos la colección en donde vamos a escribir los documentos\n",
    "#db.createCollection(name=\"delitos_carlosNieto\")\n",
    "\n",
    "# Cargamos nuestro documento json\n",
    "with open('denuncias-victimas.json', 'r') as f:\n",
    "    data = js.load(f)\n",
    "\n",
    "# Hacemos la query para insertar cada documento en nuestro json en nuestra colección\n",
    "bind = {\"dat\": data}\n",
    "aql = \"FOR d IN @dat INSERT d INTO delitos_carlosNieto\"\n",
    "queryResult = db.AQLQuery(aql, bindVars=bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
